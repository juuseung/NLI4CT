{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/CSE635proj\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets\n",
    "import collections\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import get_scheduler\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple TF-IDF approach\n",
    "test performance of a simple TF-IDF approach provided by SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.json', 'r', encoding='utf-8') as json_file:\n",
    "    dev = json.load(json_file)\n",
    "uuid_list = list(dev.keys())\n",
    "statements = []\n",
    "\n",
    "for i in range(len(uuid_list)): # retrieve all statements from the dev dataset\n",
    "  statements.append(dev[uuid_list[i]][\"Statement\"])\n",
    "\n",
    "# TF-IDF Entailment prediction baseline\n",
    "Results = {}\n",
    "\n",
    "for i in range(len(uuid_list)):\n",
    "  with open('CT json/'+dev[uuid_list[i]][\"Primary_id\"]+\".json\", 'r', encoding='utf-8') as json_file:\n",
    "    primary_ctr = json.load(json_file)\n",
    "      \n",
    "  primary_section = primary_ctr[dev[uuid_list[i]][\"Section_id\"]]\n",
    "  vectorizer = TfidfVectorizer().fit(primary_section)\n",
    "    \n",
    "  X_s = vectorizer.transform([statements[i]])\n",
    "  X_p = vectorizer.transform(primary_section)\n",
    "  primary_scores = cosine_distances(X_s, X_p)\n",
    "\n",
    "  if dev[uuid_list[i]][\"Type\"] == \"Comparison\":\n",
    "    with open('CT json/'+dev[uuid_list[i]][\"Secondary_id\"]+\".json\", 'r', encoding='utf-8') as json_file:\n",
    "      secondary_ctr = json.load(json_file)\n",
    "        \n",
    "    secondary_section = secondary_ctr[dev[uuid_list[i]][\"Section_id\"]]\n",
    "    vectorizer = TfidfVectorizer().fit(secondary_section)\n",
    "      \n",
    "    X_s = vectorizer.transform([statements[i]])\n",
    "    X_p = vectorizer.transform(secondary_section)\n",
    "    secondary_scores = cosine_distances(X_s, X_p)\n",
    "      \n",
    "    # Combine and average the cosine distances of all entries from the relevant section of the primary and secondary trial\n",
    "    combined_scores = []\n",
    "    combined_scores.extend(secondary_scores[0])\n",
    "    combined_scores.extend(primary_scores[0])\n",
    "    score = numpy.average(combined_scores)\n",
    "      \n",
    "    #If the cosine distance is gless than 0.9 the prediction is entailment\n",
    "    if score > 0.9:\n",
    "      Prediction = \"Contradiction\"\n",
    "    else:\n",
    "      Prediction = \"Entailment\"\n",
    "    Results[str(uuid_list[i])] = {\"Prediction\":Prediction}\n",
    "  else:\n",
    "    #If the cosine distance is greater than 0.9 the prediction is contradiction\n",
    "    score = numpy.average(primary_scores)\n",
    "    if score > 0.9:\n",
    "      Prediction = \"Contradiction\"\n",
    "    else:\n",
    "      Prediction = \"Entailment\"\n",
    "    Results[str(uuid_list[i])] = {\"Prediction\":Prediction}\n",
    "\n",
    "# evaluation\n",
    "gold = dev\n",
    "results = Results\n",
    "uuid_list = list(results.keys())\n",
    "\n",
    "results_pred = []\n",
    "gold_labels = []\n",
    "for i in range(len(uuid_list)):\n",
    "    if results[uuid_list[i]][\"Prediction\"] == \"Entailment\":\n",
    "        results_pred.append(1)\n",
    "    else:\n",
    "        results_pred.append(0)\n",
    "    if gold[uuid_list[i]][\"Label\"] == \"Entailment\":\n",
    "        gold_labels.append(1)\n",
    "    else:\n",
    "        gold_labels.append(0)\n",
    "\n",
    "f_score = f1_score(gold_labels,results_pred)\n",
    "p_score = precision_score(gold_labels,results_pred)\n",
    "r_score = recall_score(gold_labels,results_pred)\n",
    "\n",
    "print('F1:{:f}'.format(f_score))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.json', 'r', encoding='utf-8') as json_file:\n",
    "    dev = json.load(json_file)\n",
    "\n",
    "uuid_list = list(dev.keys())\n",
    "statements = []\n",
    "for i in range(len(uuid_list)): # retrieve all statements from the dev dataset\n",
    "  statements.append(dev[uuid_list[i]][\"Statement\"])\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cnut1648/biolinkbert-large-mnli-snli\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cnut1648/biolinkbert-large-mnli-snli\").to(device)\n",
    "\n",
    "results = {}\n",
    "for i in range(len(uuid_list)):\n",
    "    with open('CT json/'+dev[uuid_list[i]][\"Primary_id\"]+\".json\", 'r', encoding='utf-8') as json_file:\n",
    "        primary_ctr = json.load(json_file)\n",
    "    primary_section = primary_ctr[dev[uuid_list[i]][\"Section_id\"]]\n",
    "\n",
    "    statements_repeat = [statements[i]] * len(primary_section)\n",
    "    inputs = tokenizer(primary_section, statements_repeat, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "    prediction = \"Contradiction\" if predicted_class == 0 else \"Entailment\"\n",
    "\n",
    "    results[str(uuid_list[i])] = {\"Prediction\": prediction}\n",
    "\n",
    "gold = dev\n",
    "uuid_list = list(results.keys())\n",
    "results_pred = []\n",
    "gold_labels = []\n",
    "misclassified_examples = []\n",
    "\n",
    "for i in range(len(uuid_list)):\n",
    "    if results[uuid_list[i]][\"Prediction\"] == \"Entailment\":\n",
    "        results_pred.append(1)\n",
    "    else:\n",
    "        results_pred.append(0)\n",
    "    if gold[uuid_list[i]][\"Label\"] == \"Entailment\":\n",
    "        gold_labels.append(1)\n",
    "    else:\n",
    "        gold_labels.append(0)\n",
    "\n",
    "f_score = f1_score(gold_labels, results_pred, zero_division='warn')\n",
    "p_score = precision_score(gold_labels, results_pred, zero_division='warn')\n",
    "r_score = recall_score(gold_labels, results_pred, zero_division='warn')\n",
    "\n",
    "print('F1:{:f}'.format(f_score))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))\n",
    "\n",
    "conf_matrix = confusion_matrix(gold_labels, results_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "TP = conf_matrix[1, 1]\n",
    "\n",
    "print(f'True Negatives (TN): {TN}')\n",
    "print(f'False Positives (FP): {FP}')\n",
    "print(f'False Negatives (FN): {FN}')\n",
    "print(f'True Positives (TP): {TP}')\n",
    "\n",
    "class_labels = [\"Contradiction\", \"Entailment\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_labels)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finetuned BioLinkBERT - Simple Test on Dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r', encoding='utf-8') as json_file:\n",
    "  train_data = json.load(json_file)\n",
    "\n",
    "statements = []\n",
    "labels = []\n",
    "\n",
    "for entry_id, entry_data in train_data.items():\n",
    "  statements.append(entry_data['Statement'])\n",
    "  labels.append(entry_data['Label'])\n",
    "\n",
    "label_map = {'Entailment': 0, 'Contradiction': 1}\n",
    "labels_mapped = [label_map[label] for label in labels]\n",
    "\n",
    "train_stmt_tmp, test_stmt, train_label_tmp, test_label = train_test_split(statements, labels_mapped, test_size=0.2, random_state=42)\n",
    "train_stmt, valid_stmt, train_label, valid_label = train_test_split(train_stmt_tmp, train_label_tmp, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenized_train_stmt = tokenizer(train_stmt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_valid_stmt = tokenizer(valid_stmt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_test_stmt = tokenizer(test_stmt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TensorDataset(tokenized_train_stmt['input_ids'], tokenized_train_stmt['attention_mask'], torch.tensor(train_label))\n",
    "valid_dataset = TensorDataset(tokenized_valid_stmt['input_ids'], tokenized_valid_stmt['attention_mask'], torch.tensor(valid_label))\n",
    "test_dataset = TensorDataset(tokenized_test_stmt['input_ids'], tokenized_test_stmt['attention_mask'], torch.tensor(test_label))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-05)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=10*len(train_dataloader))\n",
    "\n",
    "# Training with num of epochs = 10\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  print(f'------------------------------[EPOCH {epoch}]------------------------------')\n",
    "  model.train()\n",
    "  train_loss_list = []\n",
    "  train_correct = 0\n",
    "  train_samples = 0\n",
    "  for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device), labels=batch[2].to(device))\n",
    "    train_loss = outputs.loss\n",
    "    train_loss_list.append(train_loss.item())\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    pred = torch.argmax(outputs.logits, axis=1)\n",
    "    train_correct += torch.sum(pred==batch[2].to(device)).item()\n",
    "    train_samples += len(batch[2].to(device))\n",
    "\n",
    "    if train_samples % 10 == 0:\n",
    "      average_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "      accuracy = train_correct / train_samples\n",
    "      print(f\"Batch [{train_samples}/{len(train_dataset)}] Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "  epoch_train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "  epoch_train_acc = train_correct / train_samples\n",
    "  print(f'train loss: {epoch_train_loss:.4f} | train accuracy: {epoch_train_acc:.4f}')\n",
    "\n",
    "  model.eval()\n",
    "  valid_loss_list = []\n",
    "  valid_correct = 0\n",
    "  valid_samples = 0\n",
    "  for batch in valid_dataloader:\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device), labels=batch[2].to(device))\n",
    "      valid_loss = outputs.loss\n",
    "      pred = torch.argmax(outputs.logits, axis=1)\n",
    "      valid_correct += torch.sum(pred==batch[2].to(device)).item()\n",
    "      valid_samples += len(batch[2].to(device))\n",
    "      valid_loss_list.append(valid_loss.item())\n",
    "  epoch_valid_loss = sum(valid_loss_list) / len(valid_loss_list)\n",
    "  epoch_valid_acc = valid_correct / valid_samples\n",
    "  print(f'validation loss: {epoch_valid_loss:.4f} | validation accuracy: {epoch_valid_acc:.4f}')\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss_list = []\n",
    "test_correct = 0\n",
    "test_samples = 0\n",
    "for batch in test_dataloader:\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device), labels=batch[2].to(device))\n",
    "    test_loss = outputs.loss\n",
    "    pred = torch.argmax(outputs.logits, axis=1)\n",
    "    test_correct += torch.sum(pred==batch[2].to(device)).item()\n",
    "    test_samples += len(batch[2].to(device))\n",
    "    test_loss_list.append(test_loss.item())\n",
    "\n",
    "total_test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "total_test_acc = test_correct / test_samples\n",
    "print(f'test loss: {total_test_loss:.4f} | test accuracy: {total_test_acc:.4f}')\n",
    "\n",
    "# testing on dev dataset after fine-tuning with learning scheduler\n",
    "results1 = {}\n",
    "for i in range(len(uuid_list)):\n",
    "    with open('CT json/'+dev[uuid_list[i]][\"Primary_id\"]+\".json\", 'r', encoding='utf-8') as json_file:\n",
    "        primary_ctr = json.load(json_file)\n",
    "    primary_section = primary_ctr[dev[uuid_list[i]][\"Section_id\"]]\n",
    "\n",
    "    statements_repeat = [statements[i]] * len(primary_section)\n",
    "    inputs = tokenizer(primary_section, statements_repeat, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "    prediction = \"Contradiction\" if predicted_class == 0 else \"Entailment\"\n",
    "\n",
    "    results1[str(uuid_list[i])] = {\"Prediction\": prediction}\n",
    "\n",
    "gold = dev\n",
    "uuid_list = list(results1.keys())\n",
    "\n",
    "results_pred = []\n",
    "gold_labels = []\n",
    "misclassified_examples = []\n",
    "\n",
    "for i in range(len(uuid_list)):\n",
    "    if results1[uuid_list[i]][\"Prediction\"] == \"Entailment\":\n",
    "        results_pred.append(1)\n",
    "    else:\n",
    "        results_pred.append(0)\n",
    "    if gold[uuid_list[i]][\"Label\"] == \"Entailment\":\n",
    "        gold_labels.append(1)\n",
    "    else:\n",
    "        gold_labels.append(0)\n",
    "\n",
    "f_score = f1_score(gold_labels, results_pred, zero_division='warn')\n",
    "p_score = precision_score(gold_labels, results_pred, zero_division='warn')\n",
    "r_score = recall_score(gold_labels, results_pred, zero_division='warn')\n",
    "\n",
    "print('F1:{:f}'.format(f_score))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))\n",
    "\n",
    "conf_matrix1 = confusion_matrix(gold_labels, results_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "TN = conf_matrix1[0, 0]\n",
    "FP = conf_matrix1[0, 1]\n",
    "FN = conf_matrix1[1, 0]\n",
    "TP = conf_matrix1[1, 1]\n",
    "\n",
    "print(f'True Negatives (TN): {TN}')\n",
    "print(f'False Positives (FP): {FP}')\n",
    "print(f'False Negatives (FN): {FN}')\n",
    "print(f'True Positives (TP): {TP}')\n",
    "\n",
    "class_labels = [\"Contradiction\", \"Entailment\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix1, display_labels=class_labels)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT - Simple Test on Dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "with open('dev.json', 'r', encoding='utf-8') as json_file:\n",
    "    dev = json.load(json_file)\n",
    "\n",
    "uuid_list = list(dev.keys())\n",
    "statements = []\n",
    "for i in range(len(uuid_list)): # retrieve all statements from the dev dataset\n",
    "  statements.append(dev[uuid_list[i]][\"Statement\"])\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\").to(device)\n",
    "\n",
    "results = {}\n",
    "for i in range(len(uuid_list)):\n",
    "    with open('CT json/'+dev[uuid_list[i]][\"Primary_id\"]+\".json\", 'r', encoding='utf-8') as json_file:\n",
    "        primary_ctr = json.load(json_file)\n",
    "    primary_section = primary_ctr[dev[uuid_list[i]][\"Section_id\"]]\n",
    "\n",
    "    statements_repeat = [statements[i]] * len(primary_section)\n",
    "    inputs = tokenizer(primary_section, statements_repeat, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "    prediction = \"Contradiction\" if predicted_class == 0 else \"Entailment\"\n",
    "\n",
    "    results[str(uuid_list[i])] = {\"Prediction\": prediction}\n",
    "\n",
    "gold = dev\n",
    "uuid_list = list(results.keys())\n",
    "results_pred = []\n",
    "gold_labels = []\n",
    "misclassified_examples = []\n",
    "\n",
    "for i in range(len(uuid_list)):\n",
    "    if results[uuid_list[i]][\"Prediction\"] == \"Entailment\":\n",
    "        results_pred.append(1)\n",
    "    else:\n",
    "        results_pred.append(0)\n",
    "    if gold[uuid_list[i]][\"Label\"] == \"Entailment\":\n",
    "        gold_labels.append(1)\n",
    "    else:\n",
    "        gold_labels.append(0)\n",
    "\n",
    "f_score = f1_score(gold_labels, results_pred, zero_division='warn')\n",
    "p_score = precision_score(gold_labels, results_pred, zero_division='warn')\n",
    "r_score = recall_score(gold_labels, results_pred, zero_division='warn')\n",
    "\n",
    "print('F1:{:f}'.format(f_score))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))\n",
    "\n",
    "conf_matrix = confusion_matrix(gold_labels, results_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "TP = conf_matrix[1, 1]\n",
    "\n",
    "print(f'True Negatives (TN): {TN}')\n",
    "print(f'False Positives (FP): {FP}')\n",
    "print(f'False Negatives (FN): {FN}')\n",
    "print(f'True Positives (TP): {TP}')\n",
    "\n",
    "class_labels = [\"Contradiction\", \"Entailment\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_labels)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finetuned BERT - Simple Test on Dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r', encoding='utf-8') as json_file:\n",
    "  train_data = json.load(json_file)\n",
    "\n",
    "statements = []\n",
    "labels = []\n",
    "\n",
    "for entry_id, entry_data in train_data.items():\n",
    "  statements.append(entry_data['Statement'])\n",
    "  labels.append(entry_data['Label'])\n",
    "\n",
    "label_map = {'Entailment': 0, 'Contradiction': 1}\n",
    "labels_mapped = [label_map[label] for label in labels]\n",
    "\n",
    "train_stmt_tmp, test_stmt, train_label_tmp, test_label = train_test_split(statements, labels_mapped, test_size=0.2, random_state=42)\n",
    "train_stmt, valid_stmt, train_label, valid_label = train_test_split(train_stmt_tmp, train_label_tmp, test_size=0.2, random_state=42)\n",
    "\n",
    "tokenized_train_stmt = tokenizer(train_stmt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_valid_stmt = tokenizer(valid_stmt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokenized_test_stmt = tokenizer(test_stmt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TensorDataset(tokenized_train_stmt['input_ids'], tokenized_train_stmt['attention_mask'], torch.tensor(train_label))\n",
    "valid_dataset = TensorDataset(tokenized_valid_stmt['input_ids'], tokenized_valid_stmt['attention_mask'], torch.tensor(valid_label))\n",
    "test_dataset = TensorDataset(tokenized_test_stmt['input_ids'], tokenized_test_stmt['attention_mask'], torch.tensor(test_label))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=10*len(train_dataloader))\n",
    "\n",
    "# Training with num of epochs = 10\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "  print(f'------------------------------[EPOCH {epoch}]------------------------------')\n",
    "  model.train()\n",
    "  train_loss_list = []\n",
    "  train_correct = 0\n",
    "  train_samples = 0\n",
    "  for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device), labels=batch[2].to(device))\n",
    "    train_loss = outputs.loss\n",
    "    train_loss_list.append(train_loss.item())\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    pred = torch.argmax(outputs.logits, axis=1)\n",
    "    train_correct += torch.sum(pred==batch[2].to(device)).item()\n",
    "    train_samples += len(batch[2].to(device))\n",
    "\n",
    "    if train_samples % 10 == 0:\n",
    "      average_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "      accuracy = train_correct / train_samples\n",
    "      print(f\"Batch [{train_samples}/{len(train_dataset)}] Average Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "  epoch_train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "  epoch_train_acc = train_correct / train_samples\n",
    "  print(f'train loss: {epoch_train_loss:.4f} | train accuracy: {epoch_train_acc:.4f}')\n",
    "\n",
    "  model.eval()\n",
    "  valid_loss_list = []\n",
    "  valid_correct = 0\n",
    "  valid_samples = 0\n",
    "  for batch in valid_dataloader:\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device), labels=batch[2].to(device))\n",
    "      valid_loss = outputs.loss\n",
    "      pred = torch.argmax(outputs.logits, axis=1)\n",
    "      valid_correct += torch.sum(pred==batch[2].to(device)).item()\n",
    "      valid_samples += len(batch[2].to(device))\n",
    "      valid_loss_list.append(valid_loss.item())\n",
    "  epoch_valid_loss = sum(valid_loss_list) / len(valid_loss_list)\n",
    "  epoch_valid_acc = valid_correct / valid_samples\n",
    "  print(f'validation loss: {epoch_valid_loss:.4f} | validation accuracy: {epoch_valid_acc:.4f}')\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss_list = []\n",
    "test_correct = 0\n",
    "test_samples = 0\n",
    "for batch in test_dataloader:\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids=batch[0].to(device), attention_mask=batch[1].to(device), labels=batch[2].to(device))\n",
    "    test_loss = outputs.loss\n",
    "    pred = torch.argmax(outputs.logits, axis=1)\n",
    "    test_correct += torch.sum(pred==batch[2].to(device)).item()\n",
    "    test_samples += len(batch[2].to(device))\n",
    "    test_loss_list.append(test_loss.item())\n",
    "\n",
    "total_test_loss = sum(test_loss_list) / len(test_loss_list)\n",
    "total_test_acc = test_correct / test_samples\n",
    "print(f'test loss: {total_test_loss:.4f} | test accuracy: {total_test_acc:.4f}')\n",
    "\n",
    "# testing on dev dataset after fine-tuning with learning scheduler\n",
    "results1 = {}\n",
    "for i in range(len(uuid_list)):\n",
    "    with open('CT json/'+dev[uuid_list[i]][\"Primary_id\"]+\".json\", 'r', encoding='utf-8') as json_file:\n",
    "        primary_ctr = json.load(json_file)\n",
    "    primary_section = primary_ctr[dev[uuid_list[i]][\"Section_id\"]]\n",
    "\n",
    "    statements_repeat = [statements[i]] * len(primary_section)\n",
    "    inputs = tokenizer(primary_section, statements_repeat, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits).item()\n",
    "    prediction = \"Contradiction\" if predicted_class == 0 else \"Entailment\"\n",
    "\n",
    "    results1[str(uuid_list[i])] = {\"Prediction\": prediction}\n",
    "\n",
    "gold = dev\n",
    "uuid_list = list(results1.keys())\n",
    "\n",
    "results_pred = []\n",
    "gold_labels = []\n",
    "misclassified_examples = []\n",
    "\n",
    "for i in range(len(uuid_list)):\n",
    "    if results1[uuid_list[i]][\"Prediction\"] == \"Entailment\":\n",
    "        results_pred.append(1)\n",
    "    else:\n",
    "        results_pred.append(0)\n",
    "    if gold[uuid_list[i]][\"Label\"] == \"Entailment\":\n",
    "        gold_labels.append(1)\n",
    "    else:\n",
    "        gold_labels.append(0)\n",
    "\n",
    "f_score = f1_score(gold_labels, results_pred, zero_division='warn')\n",
    "p_score = precision_score(gold_labels, results_pred, zero_division='warn')\n",
    "r_score = recall_score(gold_labels, results_pred, zero_division='warn')\n",
    "\n",
    "print('F1:{:f}'.format(f_score))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))\n",
    "\n",
    "conf_matrix1 = confusion_matrix(gold_labels, results_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "TN = conf_matrix1[0, 0]\n",
    "FP = conf_matrix1[0, 1]\n",
    "FN = conf_matrix1[1, 0]\n",
    "TP = conf_matrix1[1, 1]\n",
    "\n",
    "print(f'True Negatives (TN): {TN}')\n",
    "print(f'False Positives (FP): {FP}')\n",
    "print(f'False Negatives (FN): {FN}')\n",
    "print(f'True Positives (TP): {TP}')\n",
    "\n",
    "class_labels = [\"Contradiction\", \"Entailment\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix1, display_labels=class_labels)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ug76bFSyjM_w"
   },
   "source": [
    "# GPT-3.5-turbo  baseline\n",
    "\n",
    "A default gpt3.5-turbo model that receive a prompt and gives its responsse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CTTq-8fZ0GXA"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "client = openai.OpenAI(api_key='xxxx') #personal openai API key\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo-0125\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3HIS2iXwi7J"
   },
   "source": [
    "Simple test on Dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXgb0GbpxaeD"
   },
   "outputs": [],
   "source": [
    "dev = open('data/dev.json')\n",
    "data = json.load(dev)\n",
    "for i,(key,value) in enumerate(data.items()):\n",
    "  if value['Type']=='Single':\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT = f'data/CT json/{primary}.json'\n",
    "    CT = json.load(open(file_CT))\n",
    "    prompt = f'''\n",
    "      Based on the file and statement, determine whether the statement is a Contradiction or Entailment.\n",
    "\n",
    "      File:\n",
    "      ```{CT[section]}```\n",
    "      Statement:\n",
    "      ```{statement}```\n",
    "      '''\n",
    "    response = get_completion(prompt)\n",
    "    print('True label: ',label)\n",
    "    print('Response: ',response)\n",
    "  else:\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    secondary = value['Secondary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT1 = f'data/CT json/{primary}.json'\n",
    "    CT1 = json.load(open(file_CT1))\n",
    "    file_CT2 = f'data/CT json/{secondary}.json'\n",
    "    CT2 = json.load(open(file_CT2))\n",
    "    prompt = f'''\n",
    "      Based on the file and statement, determine whether the statement is a Contradiction or Entailment.\n",
    "\n",
    "      File:\n",
    "      ```{CT1[section]}```\n",
    "      ```{CT2[section]}```\n",
    "      Statement:\n",
    "      ```{statement}```\n",
    "      '''\n",
    "    print('True label: ',label)\n",
    "    print('Response: ',response)\n",
    "  if i>20:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igt3bnJJysrW"
   },
   "source": [
    "# Prompt engineering\n",
    "This section includes many alterations to the api setting and the prompt content, and most of the past alterations can not be preserved in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1Z91CaK0HcL"
   },
   "source": [
    "Single case development area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSxH7DD28IW4"
   },
   "outputs": [],
   "source": [
    "role = 'You are an assistant that is good at analyzing clinical trial records. When you receive a statement that makes a claim about a trial or the relation between two trials, you will read the provided information for the trials and determine whether the information support or contradict the statement.'\n",
    "\n",
    "#Predefining the role for the system\n",
    "def get_completion1(prompt, model=\"gpt-3.5-turbo-0125\"):\n",
    "    messages = [{'role': 'system', 'content': role},{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "section_description = {'Eligibility':'a set of conditions for patients to be allowed to take part in the clinical trial.',\n",
    "                          'Intervention': 'information concerning the treatment type, dosage, frequency, and duration being studied.',\n",
    "                          'Results': 'number of participants in the trial, outcome measures, units, and the results.',\n",
    "                          'Adverse Events': 'signs and symptoms observed in patients during the clinical trial.'}\n",
    "section = 'Results'\n",
    "file = f'''\n",
    "    [\n",
    "        \"Outcome Measurement: \",\n",
    "        \"  Event-free Survival\",\n",
    "        \"  Event free survival, the primary endpoint of this study, is defined as the time from randomization to the time of documented locoregional or distant recurrence, new primary breast cancer, or death from any cause.\",\n",
    "        \"  Time frame: 5 years\",\n",
    "        \"Results 1: \",\n",
    "        \"  Arm/Group Title: Exemestane\",\n",
    "        \"  Arm/Group Description: Patients receive oral exemestane (25 mg) once daily for 5 years.\",\n",
    "        \"  exemestane: Given orally\",\n",
    "        \"  Overall Number of Participants Analyzed: 3789\",\n",
    "        \"  Measure Type: Number\",\n",
    "        \"  Unit of Measure: percentage of participants  88        (87 to 89)\",\n",
    "        \"Results 2: \",\n",
    "        \"  Arm/Group Title: Anastrozole\",\n",
    "        \"  Arm/Group Description: Patients receive oral anastrozole (1 mg) once daily for 5 years.\",\n",
    "        \"  anastrozole: Given orally\",\n",
    "        \"  Overall Number of Participants Analyzed: 3787\",\n",
    "        \"  Measure Type: Number\",\n",
    "        \"  Unit of Measure: percentage of participants  89        (88 to 90)\"\n",
    "    ]\n",
    "'''\n",
    "prompt = f\"\"\"\n",
    "Your task is to determine whether a given file supports or contradicts a statement.\n",
    "\n",
    "Specifically, you should perform the following actions:\n",
    "1 - Read the following json file delimited by triple backticks. This file is the {section} of a primary trial, which may contains two cohorts.\n",
    "2 - Read the statement: \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "3 - Determine whether the file is an entailment or a contradiction to the statement. Please note that the difference between numerical numbers count as a contradiction. Provide the confidence in percentage of your answer, and the reason you make this decision.\n",
    "\n",
    "Use the following format:\n",
    "Result:\n",
    "```\n",
    "Entailment or Contradiction\n",
    "```\n",
    "Confidence:\n",
    "```\n",
    "confidence here\n",
    "```\n",
    "Reason:\n",
    "```\n",
    "Your reason here\n",
    "```\n",
    "\n",
    "File:\n",
    "```{file}```\n",
    "\"\"\"\n",
    "\n",
    "statement = 'there is a 13% difference between the results from the two the primary trial cohorts'\n",
    "prompt1 = f\"\"\"\n",
    "statement: {statement}\n",
    "File: {file}\n",
    "Is the statement a contradiction or an entailment of the file?\n",
    "\"\"\"\n",
    "prompt2 = f\"\"\"\n",
    "Perform the following actions:\n",
    "  1 - Read the following file delimited by triple backticks.\n",
    "  2 - The file contains {section_description[section]}\n",
    "  3 - Read the statement delimited by triple backticks.\n",
    "  4 - Determine whether the statement supports the file or contradicts the file.\n",
    "\n",
    "What is your reason?\n",
    "File:\n",
    "```{file}```\n",
    "Statement:\n",
    "```{statement}```\n",
    "\n",
    "\"\"\"\n",
    "# Provide your answer with a single word:\n",
    "  # Entailment or Contradiction\n",
    "# Is the statement an entailment or a contradiction of the file?\n",
    "# Determine whether the statement entails the file or contradicts the file.\n",
    "# Determine the inference relation between the statement and the file.\n",
    "response = get_completion1(prompt2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mFJHYOd0s1V"
   },
   "source": [
    "Evaluating on the dev datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a0uk1oQvO0Y"
   },
   "outputs": [],
   "source": [
    "dev = open('data/dev.json')\n",
    "data = json.load(dev)\n",
    "results_single = []\n",
    "results_comparison = []\n",
    "section_description = {'Eligibility':'a set of conditions for patients to be allowed to take part in the clinical trial.',\n",
    "                          'Intervention': 'information concerning the treatment type, dosage, frequency, and duration being studied.',\n",
    "                          'Results': 'number of participants in the trial, outcome measures, units, and the results.',\n",
    "                          'Adverse Events': 'signs and symptoms observed in patients during the clinical trial.'}\n",
    "for i,(key,value) in enumerate(data.items()):\n",
    "  if value['Type']=='Single':\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT = f'data/CT json/{primary}.json'\n",
    "    CT = json.load(open(file_CT))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the following file delimited by triple backticks.\n",
    "      2 - The file contains {section_description[section]}\n",
    "      3 - Read the statement delimited by triple backticks.\n",
    "      4 - Determine whether the file supports or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "      Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "\n",
    "      File:\n",
    "      ```{CT[section]}```\n",
    "      Statement:\n",
    "      ```{statement}```\n",
    "      '''\n",
    "    response = get_completion(prompt)\n",
    "    # print(label,response)\n",
    "    results_single.append((label,response))\n",
    "  else:\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    secondary = value['Secondary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT1 = f'data/CT json/{primary}.json'\n",
    "    CT1 = json.load(open(file_CT1))\n",
    "    file_CT2 = f'data/CT json/{secondary}.json'\n",
    "    CT2 = json.load(open(file_CT2))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the two files delimited by triple backticks. They contain {section_description[section]}\n",
    "      2 - Read the following statement: {statement}.\n",
    "      3 - The statement is about the comparison between the primary trial and the secondary trial provided.\n",
    "      4 - Determine whether the files support or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "    Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "\n",
    "    Primary trial:\n",
    "    ```{CT1[section]}```\n",
    "    Secondary trial:\n",
    "    ```{CT2[section]}```\n",
    "    '''\n",
    "    response = get_completion(prompt)\n",
    "    # print(label,response)\n",
    "    results_comparison.append((label,response))\n",
    "  # if i>20:\n",
    "  #   break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od1Ij0r9fUG0"
   },
   "outputs": [],
   "source": [
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "TN = 0\n",
    "targets = []\n",
    "results = []\n",
    "for (l,r) in results_comparison:\n",
    "  targets.append(l)\n",
    "  results.append(r)\n",
    "  # r = r.strip()\n",
    "  if l==r:\n",
    "    if l=='Entailment':\n",
    "      TP+=1\n",
    "    else: TN+=1\n",
    "  elif l=='Entailment':\n",
    "    FN+=1\n",
    "  else: FP+=1\n",
    "\n",
    "for (l,r) in results_single:\n",
    "  targets.append(l)\n",
    "  results.append(r)\n",
    "  # r = r.strip()\n",
    "  if l==r:\n",
    "    if l=='Entailment':\n",
    "      TP+=1\n",
    "    else: TN+=1\n",
    "  elif l=='Entailment':\n",
    "    FN+=1\n",
    "  else: FP+=1\n",
    "print(TP,FP,FN,TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq62caVpynCz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(targets, results)\n",
    "ConfusionMatrixDisplay(cm,display_labels=['Contradiction','Entailment']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating with CoT on the dev datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = open('data/dev.json')\n",
    "data = json.load(dev)\n",
    "labels = []\n",
    "preds = []\n",
    "response = ''\n",
    "\n",
    "for i,(key,value) in enumerate(data.items()):\n",
    "  if value['Type']=='Single':\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT = f'CT json/{primary}.json'\n",
    "    CT = json.load(open(file_CT))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the following file delimited by triple backticks.\n",
    "      2 - The file contains {section_description[section]}\n",
    "      3 - Read the statement delimited by triple backticks.\n",
    "      4 - Determine whether the file supports or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "      Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "      Reason:\n",
    "        Your reason here\n",
    "\n",
    "      File:\n",
    "      ```{CT[section]}```\n",
    "      Statement:\n",
    "      ```{statement}```\n",
    "    '''\n",
    "    prompt_tmp = prompt\n",
    "    CoT_prompts = [\n",
    "        \"Let's think step by step\",\n",
    "        \"Think about if the statement contradicts or supports information from the file.\",\n",
    "        \"Based on your analysis, determine whether the statement as 'Contradiction' or 'Entailment'\"\n",
    "    ]\n",
    "    for p in CoT_prompts:\n",
    "      prompt_tmp = prompt_tmp + '\\n' + p\n",
    "      response = get_completion(prompt_tmp)\n",
    "\n",
    "    labels.append(label)\n",
    "    preds.append(response.split(\"Reason\")[0])\n",
    "  else:\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    secondary = value['Secondary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT1 = f'CT json/{primary}.json'\n",
    "    CT1 = json.load(open(file_CT1))\n",
    "    file_CT2 = f'CT json/{secondary}.json'\n",
    "    CT2 = json.load(open(file_CT2))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the two files delimited by triple backticks. They contain {section_description[section]}\n",
    "      2 - Read the following statement: {statement}.\n",
    "      3 - The statement is about the comparison between the primary trial and the secondary trial provided.\n",
    "      4 - Determine whether the files support or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "    Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "    Reason：\n",
    "      Your reason here\n",
    "\n",
    "    Primary trial:\n",
    "    ```{CT1[section]}```\n",
    "    Secondary trial:\n",
    "    ```{CT2[section]}```\n",
    "    '''\n",
    "    prompt_tmp = prompt\n",
    "    CoT_prompts = [\n",
    "        \"Let's think step by step\",\n",
    "        \"Think about if the statement contradicts or supports information from the files.\",\n",
    "        \"Based on your analysis, determine whether the statement as 'Contradiction' or 'Entailment'\"\n",
    "    ]\n",
    "    for p in CoT_prompts:\n",
    "      prompt_tmp = prompt_tmp + '\\n' + p\n",
    "      response = get_completion(prompt_tmp)\n",
    "\n",
    "    labels.append(label)\n",
    "    preds.append(response.split(\"Reason\")[0])\n",
    "  # if i>=0:\n",
    "  #   break\n",
    "\n",
    "label_map = {'Contradiction': 0, 'Entailment': 1}\n",
    "labels_new = [label_map[label] for label in labels]\n",
    "preds_new = [0 if 'Contradiction' in pr else 1 for pr in preds]\n",
    "\n",
    "conf_matrix_cot = confusion_matrix(labels_new, preds_new)\n",
    "f_score = f1_score(labels_new, preds_new, zero_division='warn')\n",
    "p_score = precision_score(labels_new, preds_new, zero_division='warn')\n",
    "r_score = recall_score(labels_new, preds_new, zero_division='warn')\n",
    "\n",
    "print('F1:{:f}'.format(f_score))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))\n",
    "\n",
    "class_labels = [\"Contradiction\", \"Entailment\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_cot, display_labels=class_labels)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILQSoltW675w"
   },
   "source": [
    "# Finetuning\n",
    "Training data file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ma0HCuDu_T7e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train = open('data/train.json')\n",
    "train_data = json.load(train)\n",
    "val_file = open('val.jsonl', 'w', encoding='utf8')\n",
    "train_file = open('train.jsonl', 'w', encoding='utf8')\n",
    "\n",
    "role = 'You are an assistant that is good at analyzing clinical trial records. When you receive a statement that makes a claim about a trial or the relation between two trials, you will read the provided information for the trials and determine whether the information support or contradict the statement.'\n",
    "\n",
    "\n",
    "for i,(key,value) in enumerate(train_data.items()):\n",
    "  section = value['Section_id']\n",
    "  primary = value['Primary_id']\n",
    "  statement = value['Statement']\n",
    "  label = value['Label']\n",
    "  file_CT1 = f'data/CT json/{primary}.json'\n",
    "  CT1 = json.load(open(file_CT1))\n",
    "  if value['Type']=='Single':\n",
    "    prompt = f'''\n",
    "    Statement:\n",
    "    {statement}\n",
    "    Primary trial:\n",
    "    {CT1[section]}\n",
    "    '''\n",
    "  else:\n",
    "    secondary = value['Secondary_id']\n",
    "    file_CT2 = f'data/CT json/{secondary}.json'\n",
    "    CT2 = json.load(open(file_CT2))\n",
    "    prompt = f'''\n",
    "    Statement:\n",
    "    {statement}\n",
    "    Primary trial:\n",
    "    {CT1[section]}\n",
    "    Secondary trial:\n",
    "    {CT2[section]}\n",
    "    '''\n",
    "  msgs = [{'role': 'system', 'content': role},{\"role\": \"user\", \"content\": prompt},{\"role\": \"assistant\", \"content\": label}]\n",
    "  msg = {'messages':msgs}\n",
    "  if i<=1500:\n",
    "    val_file.write(json.dumps(msg))\n",
    "    val_file.write('\\n')\n",
    "    continue\n",
    "  train_file.write(json.dumps(msg))\n",
    "  train_file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09yfADRa75Sl"
   },
   "source": [
    "Upload training files and finefune the model on OpenAI playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmOD4yi1PvnD"
   },
   "outputs": [],
   "source": [
    "client.files.create(\n",
    "  file=open(\"val.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "client.files.create(\n",
    "  file=open(\"train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylRpaQlw707v"
   },
   "source": [
    "Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxgYKPnHFE-V"
   },
   "outputs": [],
   "source": [
    "def get_completion2(prompt, model=\"ft:gpt-3.5-turbo-0125:personal:full:9JB5gM4K\"):\n",
    "    messages = [{'role': 'system', 'content': role},{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Finetuned Model with CoT on dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = open('data/dev.json')\n",
    "data = json.load(dev)\n",
    "labels = []\n",
    "preds = []\n",
    "response = ''\n",
    "\n",
    "for i,(key,value) in enumerate(data.items()):\n",
    "  if value['Type']=='Single':\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT = f'CT json/{primary}.json'\n",
    "    CT = json.load(open(file_CT))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the following file delimited by triple backticks.\n",
    "      2 - The file contains {section_description[section]}\n",
    "      3 - Read the statement delimited by triple backticks.\n",
    "      4 - Determine whether the file supports or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "      Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "      Reason:\n",
    "        Your reason here\n",
    "\n",
    "      File:\n",
    "      ```{CT[section]}```\n",
    "      Statement:\n",
    "      ```{statement}```\n",
    "    '''\n",
    "    prompt_tmp = prompt\n",
    "    CoT_prompts = [\n",
    "        \"Let's think step by step\",\n",
    "        \"Think about if the statement contradicts or supports information from the file.\",\n",
    "        \"Based on your analysis, determine whether the statement as 'Contradiction' or 'Entailment'\"\n",
    "    ]\n",
    "    for p in CoT_prompts:\n",
    "      prompt_tmp = prompt_tmp + '\\n' + p\n",
    "      response = get_completion2(prompt_tmp)\n",
    "\n",
    "    labels.append(label)\n",
    "    preds.append(response.split(\"Reason\")[0])\n",
    "  else:\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    secondary = value['Secondary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT1 = f'CT json/{primary}.json'\n",
    "    CT1 = json.load(open(file_CT1))\n",
    "    file_CT2 = f'CT json/{secondary}.json'\n",
    "    CT2 = json.load(open(file_CT2))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the two files delimited by triple backticks. They contain {section_description[section]}\n",
    "      2 - Read the following statement: {statement}.\n",
    "      3 - The statement is about the comparison between the primary trial and the secondary trial provided.\n",
    "      4 - Determine whether the files support or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "    Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "    Reason：\n",
    "      Your reason here\n",
    "\n",
    "    Primary trial:\n",
    "    ```{CT1[section]}```\n",
    "    Secondary trial:\n",
    "    ```{CT2[section]}```\n",
    "    '''\n",
    "    prompt_tmp = prompt\n",
    "    CoT_prompts = [\n",
    "        \"Let's think step by step\",\n",
    "        \"Think about if the statement contradicts or supports information from the files.\",\n",
    "        \"Based on your analysis, determine whether the statement as 'Contradiction' or 'Entailment'\"\n",
    "    ]\n",
    "    for p in CoT_prompts:\n",
    "      prompt_tmp = prompt_tmp + '\\n' + p\n",
    "      response = get_completion2(prompt_tmp)\n",
    "\n",
    "    labels.append(label)\n",
    "    preds.append(response.split(\"Reason\")[0])\n",
    "  # if i>=0:\n",
    "  #   break\n",
    "\n",
    "label_map = {'Contradiction': 0, 'Entailment': 1}\n",
    "labels_new = [label_map[label] for label in labels]\n",
    "preds_new = [0 if 'Contradiction' in pr else 1 for pr in preds]\n",
    "\n",
    "conf_matrix_cot = confusion_matrix(labels_new, preds_new)\n",
    "f_score = f1_score(labels_new, preds_new, zero_division='warn')\n",
    "p_score = precision_score(labels_new, preds_new, zero_division='warn')\n",
    "r_score = recall_score(labels_new, preds_new, zero_division='warn')\n",
    "\n",
    "print('F1:{:f}'.format(f_score))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))\n",
    "\n",
    "class_labels = [\"Contradiction\", \"Entailment\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_cot, display_labels=class_labels)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWCeza8NCchH"
   },
   "source": [
    "# Evaluate Final Model on gold_practice_test dataset\n",
    "evaluate faithfulness, consistency based on the codes provided by SemEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_practice_test = open('/content/drive/MyDrive/CSE635proj/gold_practice_test.json')\n",
    "gold_data = json.load(gold_practice_test)\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "preds_keys = []\n",
    "response = ''\n",
    "\n",
    "for i,(key,value) in enumerate(gold_data.items()):\n",
    "  if value['Type']=='Single':\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT = f'CT json/{primary}.json'\n",
    "    CT = json.load(open(file_CT))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the following file delimited by triple backticks.\n",
    "      2 - The file contains {section_description[section]}\n",
    "      3 - Read the statement delimited by triple backticks.\n",
    "      4 - Determine whether the file supports or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "      Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "      Reason:\n",
    "        Your reason here\n",
    "\n",
    "      File:\n",
    "      ```{CT[section]}```\n",
    "      Statement:\n",
    "      ```{statement}```\n",
    "    '''\n",
    "    prompt_tmp = prompt\n",
    "    CoT_prompts = [\n",
    "        \"Let's think step by step\",\n",
    "        \"Think about if the statement contradicts or supports information from the file.\",\n",
    "        \"Based on your analysis, determine whether the statement as 'Contradiction' or 'Entailment'\"\n",
    "    ]\n",
    "    for p in CoT_prompts:\n",
    "      prompt_tmp = prompt_tmp + '\\n' + p\n",
    "      response = get_completion2(prompt_tmp)\n",
    "\n",
    "    labels.append(label)\n",
    "    preds.append(response.split(\"Reason\")[0])\n",
    "    preds_keys.append(key)\n",
    "  else:\n",
    "    section = value['Section_id']\n",
    "    primary = value['Primary_id']\n",
    "    secondary = value['Secondary_id']\n",
    "    statement = value['Statement']\n",
    "    label = value['Label']\n",
    "    file_CT1 = f'CT json/{primary}.json'\n",
    "    CT1 = json.load(open(file_CT1))\n",
    "    file_CT2 = f'CT json/{secondary}.json'\n",
    "    CT2 = json.load(open(file_CT2))\n",
    "    prompt = f'''\n",
    "    Perform the following actions:\n",
    "      1 - Read the two files delimited by triple backticks. They contain {section_description[section]}\n",
    "      2 - Read the following statement: {statement}.\n",
    "      3 - The statement is about the comparison between the primary trial and the secondary trial provided.\n",
    "      4 - Determine whether the files support or contradicts the statement. If support, your answer will be Entailment; if contradict, your answer will be Contradiction.\n",
    "\n",
    "    Use the following format:\n",
    "        <Contradiction or Entailment>\n",
    "    Reason：\n",
    "      Your reason here\n",
    "\n",
    "    Primary trial:\n",
    "    ```{CT1[section]}```\n",
    "    Secondary trial:\n",
    "    ```{CT2[section]}```\n",
    "    '''\n",
    "    prompt_tmp = prompt\n",
    "    CoT_prompts = [\n",
    "        \"Let's think step by step\",\n",
    "        \"Think about if the statement contradicts or supports information from the files.\",\n",
    "        \"Based on your analysis, determine whether the statement as 'Contradiction' or 'Entailment'\"\n",
    "    ]\n",
    "    for p in CoT_prompts:\n",
    "      prompt_tmp = prompt_tmp + '\\n' + p\n",
    "      response = get_completion2(prompt_tmp)\n",
    "\n",
    "    labels.append(label)\n",
    "    preds.append(response.split(\"Reason\")[0])\n",
    "    preds_keys.append(key)\n",
    "  # if i>=0:\n",
    "  #   break\n",
    "\n",
    "label_map = {'Contradiction': 0, 'Entailment': 1}\n",
    "labels_new = [label_map[label] for label in labels]\n",
    "preds_new = [0 if 'Contradiction' in pr else 1 for pr in preds]\n",
    "preds_cleaned = ['Contradiction' if 'Contradiction' in pr else 'Entailment' for pr in preds]\n",
    "\n",
    "conf_matrix_cot = confusion_matrix(labels_new, preds_new)\n",
    "f_score_macro = f1_score(labels_new, preds_new, average='macro', zero_division='warn')\n",
    "p_score = precision_score(labels_new, preds_new, zero_division='warn')\n",
    "r_score = recall_score(labels_new, preds_new, zero_division='warn')\n",
    "\n",
    "print('F1 socre macro:{:f}'.format(f_score_macro))\n",
    "print('precision_score:{:f}'.format(p_score))\n",
    "print('recall_score:{:f}'.format(r_score))\n",
    "\n",
    "class_labels = [\"Contradiction\", \"Entailment\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_cot, display_labels=class_labels)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faithfulness(predictions, gold):\n",
    "    uuid_list = list(predictions.keys())\n",
    "    N = len(uuid_list)\n",
    "    results = []\n",
    "    for key in uuid_list:\n",
    "        if predictions[key][\"Prediction\"] != gold[gold[key][\"Causal_type\"][1]][\"Label\"]:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    Faithfulness = sum(results) / N\n",
    "    return Faithfulness\n",
    "\n",
    "\n",
    "def consistency(predictions, gold):\n",
    "    uuid_list = list(predictions.keys())\n",
    "    N = len(uuid_list)\n",
    "    results = []\n",
    "    for key in uuid_list:\n",
    "        if predictions[key][\"Prediction\"] == gold[key][\"Label\"]:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    Consistency = sum(results) / N\n",
    "    return Consistency\n",
    "\n",
    "def extract_by_causal_type(predictions, gold):\n",
    "    predictions_preserving = {}\n",
    "    predictions_altering = {}\n",
    "    for key in predictions.keys():\n",
    "        if \"Causal_type\" not in gold[key].keys():\n",
    "            continue\n",
    "        if gold[key][\"Causal_type\"][0] == \"Preserving\":\n",
    "            predictions_preserving[key] = predictions[key]\n",
    "        elif gold[key][\"Causal_type\"][0] == \"Altering\":\n",
    "            predictions_altering[key] = predictions[key]\n",
    "    return predictions_preserving, predictions_altering\n",
    "\n",
    "def extract_contrast_set(predictions, gold):\n",
    "    contrast_predicitons = {}\n",
    "    for key in predictions.keys():\n",
    "        if \"Causal_type\" in gold[key].keys():\n",
    "            contrast_predicitons[key] = predictions[key]\n",
    "    return contrast_predicitons\n",
    "\n",
    "def extract_by_intervention(predictions, gold):\n",
    "    para_predictions = {}\n",
    "    cont_predictions = {}\n",
    "    numerical_para_predictions = {}\n",
    "    numerical_cont_predictions = {}\n",
    "    definitions_predictions = {}\n",
    "    for key in predictions.keys():\n",
    "        if \"Intervention\" not in gold[key].keys():\n",
    "            continue\n",
    "        if gold[key][\"Intervention\"] == \"Paraphrase\":\n",
    "            para_predictions[key] = predictions[key]\n",
    "        elif gold[key][\"Intervention\"] == \"Contradiction\":\n",
    "            cont_predictions[key] = predictions[key]\n",
    "        elif gold[key][\"Intervention\"] == \"Numerical_paraphrase\":\n",
    "            numerical_para_predictions[key] = predictions[key]\n",
    "        elif gold[key][\"Intervention\"] == \"Numerical_contradiction\":\n",
    "            numerical_cont_predictions[key] = predictions[key]\n",
    "        elif gold[key][\"Intervention\"] == \"Text_appended\":\n",
    "            definitions_predictions[key] = predictions[key]\n",
    "    return para_predictions, cont_predictions, numerical_para_predictions, numerical_cont_predictions, definitions_predictions\n",
    "\n",
    "def F1_Recall_Precision(predictions, gold):\n",
    "    pred_labels = []\n",
    "    gold_labels = []\n",
    "    for key in predictions.keys():\n",
    "        if predictions[key][\"Prediction\"] == \"Entailment\":\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "        if gold[key][\"Label\"] == \"Entailment\":\n",
    "            gold_labels.append(1)\n",
    "        else:\n",
    "            gold_labels.append(0)\n",
    "    F1 = f1_score(gold_labels, pred_labels)\n",
    "    Recall = precision_score(gold_labels, pred_labels)\n",
    "    Precision = recall_score(gold_labels, pred_labels)\n",
    "    return F1, Recall, Precision\n",
    "\n",
    "def extract_control_set(predictions, gold):\n",
    "    control_predicitons = {}\n",
    "    for key in gold.keys():\n",
    "        if \"Causal_type\" not in gold[key].keys():\n",
    "            control_predicitons[key] = predictions[key]\n",
    "    return control_predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for key, value in zip(preds_keys, preds_cleaned):\n",
    "  predictions[key] = {\"Prediction\": value}\n",
    "\n",
    "# Control Test Set F1, Recall, Precision PUBLIC\n",
    "Control_F1, Control_Rec, Control_Prec = F1_Recall_Precision(extract_control_set(predictions, gold_data), gold_data)\n",
    "\n",
    "# Contrast Consistency & Faithfullness PUBLIC\n",
    "contrast_predictions = extract_contrast_set(predictions, gold_data)\n",
    "predictions_preserving, predictions_altering = extract_by_causal_type(contrast_predictions, gold_data)\n",
    "Faithfulness = faithfulness(predictions_altering, gold_data)\n",
    "Consistency = consistency(predictions_preserving, gold_data)\n",
    "\n",
    "\n",
    "# Intervention-wise Consistency & Faithfullness HIDDEN\n",
    "para_predictions, cont_predictions, numerical_para_predictions, numerical_cont_predictions, definitions_predictions = \\\n",
    "    extract_by_intervention(predictions, gold_data)\n",
    "para_preserving = extract_by_causal_type(para_predictions, gold_data)[0]\n",
    "cont_preserving, cont_altering = extract_by_causal_type(cont_predictions, gold_data)\n",
    "numerical_para_preserving = extract_by_causal_type(numerical_para_predictions, gold_data)[0]\n",
    "numerical_cont_preserving, numerical_cont_altering = extract_by_causal_type(numerical_cont_predictions, gold_data)\n",
    "definitions_preserving = extract_by_causal_type(definitions_predictions, gold_data)[0]\n",
    "para_Consistency = consistency(para_preserving, gold_data)\n",
    "cont_Faithfulness = faithfulness(cont_altering, gold_data)\n",
    "cont_Consistency = consistency(cont_preserving, gold_data)\n",
    "numerical_para_Consistency = consistency(numerical_para_preserving, gold_data)\n",
    "numerical_cont_Faithfulness = faithfulness(numerical_cont_altering, gold_data)\n",
    "numerical_cont_Consistency = consistency(numerical_cont_preserving, gold_data)\n",
    "definitions_Consistency = consistency(definitions_preserving, gold_data)\n",
    "\n",
    "# Intervention-wise F1, Recall, Precision HIDDEN\n",
    "Contrast_F1, Contrast_Rec, Contrast_Prec = F1_Recall_Precision(contrast_predictions, gold_data)\n",
    "para_F1, para_Rec, para_Prec = F1_Recall_Precision(para_predictions, gold_data)\n",
    "cont_F1, cont_Rec, cont_Prec = F1_Recall_Precision(cont_predictions, gold_data)\n",
    "numerical_para_F1, numerical_para_Rec, numerical_para_Prec = F1_Recall_Precision(numerical_para_predictions, gold_data)\n",
    "numerical_cont_F1, numerical_cont_Rec, numerical_cont_Prec = F1_Recall_Precision(numerical_cont_predictions, gold_data)\n",
    "definitions_F1, definitions_Rec, definitions_Prec = F1_Recall_Precision(definitions_predictions, gold_data)\n",
    "\n",
    "# Output results\n",
    "with open('scores.txt', 'w') as f:\n",
    "    print('Control_F1: ', Control_F1, file=f)\n",
    "    print('Control_Recall: ', Control_Rec, file=f)\n",
    "    print('Control_Precision: ', Control_Prec, file=f)\n",
    "    print('Contrast_F1: ', Contrast_F1, file=f)\n",
    "    print('Contrast_Recall: ', Contrast_Rec, file=f)\n",
    "    print('Contrast_Precision: ', Contrast_Prec, file=f)\n",
    "    print('Faithfulness: ', Faithfulness, file=f)\n",
    "    print('Consistency: ', Consistency, file=f)\n",
    "    print('Para_Consistency: ', para_Consistency, file=f)\n",
    "    print('Cont_Faithfulness: ', cont_Faithfulness, file=f)\n",
    "    print('Cont_Consistency: ', cont_Consistency, file=f)\n",
    "    print('Numerical_Para_Consistency: ', numerical_para_Consistency, file=f)\n",
    "    print('Numerical_Cont_Faithfulness: ', numerical_cont_Faithfulness, file=f)\n",
    "    print('Numerical_Cont_Consistency: ', numerical_cont_Consistency, file=f)\n",
    "    print('Definitions_Consistency: ', definitions_Consistency, file=f)\n",
    "    print('Para_F1: ', para_F1, file=f)\n",
    "    print('Para_Recall: ', para_Rec, file=f)\n",
    "    print('Para_Precision: ', para_Prec, file=f)\n",
    "    print('Cont_F1: ', cont_F1, file=f)\n",
    "    print('Cont_Recall: ', cont_Rec, file=f)\n",
    "    print('Cont_Precision: ', cont_Prec, file=f)\n",
    "    print('Numerical_Para_F1: ', numerical_para_F1, file=f)\n",
    "    print('Numerical_Para_Recall: ', numerical_para_Rec, file=f)\n",
    "    print('Numerical_Para_Precision: ', numerical_para_Prec, file=f)\n",
    "    print('Numerical_Cont_F1: ', numerical_cont_F1, file=f)\n",
    "    print('Numerical_Cont_Recall: ', numerical_cont_Rec, file=f)\n",
    "    print('Numerical_Cont_Precision: ', numerical_cont_Prec, file=f)\n",
    "    print('Definitions_F1: ', definitions_F1, file=f)\n",
    "    print('Definitions_Recall: ', definitions_Rec, file=f)\n",
    "    print('Definitions_Precision: ', definitions_Prec, file=f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
